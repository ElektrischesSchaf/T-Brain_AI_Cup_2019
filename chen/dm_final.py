# -*- coding: utf-8 -*-
"""DM_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JoSJdxEtKjb8onOQiK6XsrMGxaZKKb7Q
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import pickle, json, re, time

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.utils.tensorboard import SummaryWriter

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split

from tqdm import tqdm_notebook as tqdm
#from tqdm import tqdm
from tqdm import trange

from gensim.parsing import remove_stopwords

#from google.colab import drive
import os

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

dataset = pd.read_csv('../Sample_Code/task1/data/task1_trainset.csv', dtype=str)

dataset.head()

### Remove (current) redundant columns.
dataset.drop('Title',axis=1,inplace=True)
dataset.drop('Categories',axis=1,inplace=True)
dataset.drop('Created Date',axis=1, inplace=True)
dataset.drop('Authors',axis=1,inplace=True)

dataset['Abstract'] = dataset['Abstract'].str.lower()

# for i in range(len(dataset['Abstract'])):
#     dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])

# set test_size=0.1 for validation split
trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)

trainset.to_csv('../Sample_Code/task1/data/trainset.csv', index=False)
validset.to_csv('../Sample_Code/task1/data/validset.csv', index=False)

dataset = pd.read_csv('../Sample_Code/task1/data/task1_public_testset.csv', dtype=str)

dataset.drop('Title',axis=1,inplace=True)
dataset.drop('Categories',axis=1,inplace=True)
dataset.drop('Created Date',axis=1, inplace=True)
dataset.drop('Authors',axis=1,inplace=True)

dataset['Abstract'] = dataset['Abstract'].str.lower()

# for i in range(len(dataset['Abstract'])):
#     dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])


dataset.to_csv('../Sample_Code/task1/data/testset.csv', index=False)


sent_list = []
label_list = []
for i in dataset.iterrows():
    # remove $$$ and append to sent_list
    
    sent_list += i[1]['Abstract'].split('$$$')
    label_list += i[1]['Task 1'].split(' ')

df = pd.DataFrame({'Abstract': sent_list,
                   'Label': label_list})

def count_words(x):
    return len(x.split(' '))

df['num_of_words'] = df['Abstract'].apply(count_words)
df.head()

df.describe()

plt.figure(figsize=(12, 7))
plt.plot(df['num_of_words'], marker='.', linestyle='')

def label_to_onehot(labels):
    """ Convert label to onehot .
        Args:
            labels (string): sentence's labels.
        Return:
            outputs (onehot list): sentence's onehot label.
    """
    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}
    onehot = [0,0,0,0,0,0]
    for l in labels.split('/'):
        onehot[label_dict[l]] = 1
    return tuple(onehot)

df['Onehot'] = df['Label'].apply(label_to_onehot)

df = df.loc[:, ['Abstract', 'Onehot']]
df.rename(columns={'Abstract': 0, 'Onehot': 1}, inplace=True)
df.head()

trainset, validset = train_test_split(df, test_size=0.1, random_state=42)

#pip install simpletransformers

from simpletransformers.classification import MultiLabelClassificationModel

model = MultiLabelClassificationModel('roberta', 
                                      'roberta-base',
                                      num_labels=6, 
                                      args={'output_dir': 'outputs/',
                                            'cache_dir': 'cache/',
                                            # 'fp16': True,
                                            # 'fp16_opt_level': 'O1',
                                            'max_seq_length': 70,
                                            'train_batch_size': 16,
                                            'eval_batch_size': 16,
                                            'gradient_accumulation_steps': 1,
                                            'num_train_epochs': 7,
                                            'weight_decay': 0,
                                            'learning_rate': 4e-5,
                                            'adam_epsilon': 1e-8,
                                            'warmup_ratio': 0.06,
                                            'warmup_steps': 0,
                                            'max_grad_norm': 1.0,
                                            
                                            'logging_steps': 50,
                                            'evaluate_during_training': False,
                                            'save_steps': 2000,
                                            'eval_all_checkpoints': True,
                                            'use_tensorboard': True,

                                            'overwrite_output_dir': False,
                                            'reprocess_input_data': False,
                                            
                                            # 'process_count': cpu_count() - 2 if cpu_count() > 2 else 1,
                                            'n_gpu': 1,
                                            'silent': False,
                                            'use_multiprocessing': True})

model.train_model(trainset)

result, model_outputs, wrong_predictions = model.eval_model(validset)

result

model_outputs

# Load saved model from 'outputs/'
# model = MultiLabelClassificationModel('roberta', 'outputs/')

dataset = pd.read_csv('../Sample_Code/task1/data/trainset.csv', dtype=str)

sent_list = []
sid_list = []
limit = 0

for index, row in testset.iterrows():
    # remove $$$ and append to sent_list
    new_sent = row['Abstract'].split('$$$')
    sent_list += new_sent
    # Construct sid_list
    N = len(new_sent) + 1
    for i in range(1, N):
        sid = '%s_S%.3d' % (row['Id'], i)
        sid_list.append(sid)
    # limit = limit + 1
    # if limit > 100:
    #     break

len(sent_list), len(sid_list)

preds, outputs = model.predict(sent_list)

outputs

submit_df = pd.DataFrame({'order_id': sid_list,
                           'BACKGROUND': None,
                           'OBJECTIVES': None,
                           'METHODS': None,
                           'RESULTS': None,
                           'CONCLUSIONS': None,
                           'OTHERS': None,
                           'preds': preds})

submit_df['BACKGROUND'] = submit_df['preds'].apply(lambda x: x[0])
submit_df['OBJECTIVES'] = submit_df['preds'].apply(lambda x: x[1])
submit_df['METHODS'] = submit_df['preds'].apply(lambda x: x[2])
submit_df['RESULTS'] = submit_df['preds'].apply(lambda x: x[3])
submit_df['CONCLUSIONS'] = submit_df['preds'].apply(lambda x: x[4])
submit_df['OTHERS'] = submit_df['preds'].apply(lambda x: x[5])
submit_df.drop(['preds'], axis=1, inplace=True)

private_testset = pd.read_csv('../Sample_Code/task1/data/task1_sample_submission.csv')

private_testset = private_testset.iloc[131166:, :]
private_testset.head()

submit_df = pd.concat([submit_df, private_testset])
submit_df.tail()

submit_df.to_csv('submit2.csv', index=False)

test_df = pd.DataFrame({'Abstract': sent_list})

def count_words(x):
    return len(x.split(' '))

test_df['num_of_words'] = test_df['Abstract'].apply(count_words)

test_df.describe()

plt.figure(figsize=(12, 7))
plt.plot(test_df['num_of_words'], marker='.', linestyle='')
plt.savefig('results_plot.png')
